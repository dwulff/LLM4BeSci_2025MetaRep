{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58cef844cd9b0ecd",
      "metadata": {
        "collapsed": false,
        "id": "58cef844cd9b0ecd"
      },
      "source": [
        "# Day 2b\n",
        "In this notebook, we will continue with tweet bias classification. However, we will now use feature extraction instead of zero-/few-shot classification. We will use the `SentenceTransformer` library to extract features from the text, and then use a `RidgeClassifierCV` to classify the tweets. We will train the classifier on some additional training data (`media_bias_train`) and evaluate it on the same data as day_3a (`media_bias_test`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2d0673bbb432e4f",
      "metadata": {
        "collapsed": false,
        "id": "e2d0673bbb432e4f"
      },
      "source": [
        "## Environment Setup\n",
        "Make sure to set your runtime back to using CPU by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `CPU`. This will save you some GPU hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "20c32f75-3fd7-415f-b30f-8cadb2827a48"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
        "    # Mount google drive to enable access to data files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install requisite packages\n",
        "    !pip install sentence_transformers &> /dev/null\n",
        "\n",
        "    # Change working directory to day_3\n",
        "    %cd /content/drive/MyDrive/LLM4BeSci_2025MetaRep/day_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee02d097856ee15",
      "metadata": {
        "id": "5ee02d097856ee15"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde917b91e26fd55",
      "metadata": {
        "collapsed": false,
        "id": "bde917b91e26fd55"
      },
      "source": [
        "## Feature Extraction\n",
        "The code begins by loading the data as `pandas.DataFrame` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f06a2fe2cf2038d",
      "metadata": {
        "id": "6f06a2fe2cf2038d"
      },
      "outputs": [],
      "source": [
        "# Reload test data from last last notebook (day_3a.ipynb)\n",
        "media_bias_test = pd.read_csv('media_bias_test.csv')\n",
        "\n",
        "# Load training data\n",
        "media_bias_train = pd.read_csv('media_bias_train.csv')\n",
        "media_bias_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a9bca646cd9146",
      "metadata": {
        "collapsed": false,
        "id": "37a9bca646cd9146"
      },
      "source": [
        "Note the considerable increase in the number of training samples. The code then next initializes the `SentenceTransformer` model `'all-mpnet-base-v2'` and extracts features from the training data using the `encode` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c60ffe89614a76f",
      "metadata": {
        "id": "8c60ffe89614a76f"
      },
      "outputs": [],
      "source": [
        "# Initialize feature extraction pipeline\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Extract features\n",
        "train_features = model.encode(media_bias_train['text'])\n",
        "train_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc1bca83056a9f4",
      "metadata": {
        "collapsed": false,
        "id": "2cc1bca83056a9f4"
      },
      "source": [
        "\n",
        "The features are then standardised before being fed into `RidgeClassifierCV`. This is crucial, since `RidgeClassifierCV` uses l2 (ridge) regularisation to prevent over-fitting, which assumes that the features have the same scaling. The classifier is then trained on the training data using the `fit` method. Note that `RidgeClassifierCV` will automatically perform cross-validation on the training data to find the best alpha value from the list of `alphas` provided. Performance on the training data is then evaluated using the `score` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "isODrTlzhp5E",
      "metadata": {
        "id": "isODrTlzhp5E"
      },
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_features)\n",
        "features = scaler.transform(train_features)\n",
        "\n",
        "# Initialize classifier\n",
        "ridge = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
        "\n",
        "# Train classifier\n",
        "ridge.fit(train_features, media_bias_train['bias'])\n",
        "f\"Train accuracy: {ridge.score(train_features, media_bias_train['bias'])}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6534cd57d8073d56",
      "metadata": {
        "collapsed": false,
        "id": "6534cd57d8073d56"
      },
      "source": [
        "Features are next extracted for the test set and standardised using the same `StandardScaler` object that was fitted on the training data to prevent data leakage. The classifier is then evaluated on the test data using the `score` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2814c31f5ab1686",
      "metadata": {
        "id": "f2814c31f5ab1686"
      },
      "outputs": [],
      "source": [
        "# Extract features for test set\n",
        "test_features = model.encode(media_bias_test['text'])\n",
        "\n",
        "# Standardising features\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "# Test classifier\n",
        "f\"Test accuracy: {ridge.score(test_features, media_bias_test['bias'])}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d71a280d578bcd",
      "metadata": {
        "collapsed": false,
        "id": "b6d71a280d578bcd"
      },
      "source": [
        "As you can see, feature extraction outperforms zero-shot and few-shot classification from the last notebook. Why do you think this is?\n",
        "\n",
        "We can also visualize the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738e5d8eb936d057",
      "metadata": {
        "id": "738e5d8eb936d057"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "confusion = pd.crosstab(media_bias_test['bias'], ridge.predict(test_features))\n",
        "sns.heatmap(confusion, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de852e7f71c942f",
      "metadata": {
        "collapsed": false,
        "id": "4de852e7f71c942f"
      },
      "source": [
        "Like with few-shot, the feature extraction approach identifies more neutral tweets as partisan (false positives) than it does partisan tweets as neutral (false negatives).\n",
        "\n",
        "**TASK 1:** Go to the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) and find a well-performing small model (i.e., high in the leaderboard, <1 billion parameters). Open the model card by clicking on the model, test whether the model can be run with `sentence-transformers` (by looking at the tags under the model name: there should be tag called `sentence-transformers`). Replace the `\"all-mpnet-base-v2\"` in the code above and re-run the analysis. Does the performance improve?\n",
        "\n",
        "**TASK 2:** The few shot performance measure we have so far been using is a single point estimate. Can you think of a way to get an uncertainty estimate on the test performance (e.g., a confidence interval)? Hint: Think along the lines of bootstrapping."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jnefIZpf2DyN",
      "metadata": {
        "id": "jnefIZpf2DyN"
      },
      "source": [
        "##**BONUS - LoRA Fine-Tuning (MiniLM)**\n",
        "\n",
        "So far, we have treated the language model as a fixed feature extractor:\n",
        "the model produces embeddings, and all task learning happens in a separate, linear classifier.\n",
        "\n",
        "Now we allow the language model itself to **adapt to the task**.\n",
        "\n",
        "A straightforward way to do this would be full fine-tuning, where all model parameters are updated.\n",
        "\n",
        "However, full fine-tuning: (1) updates millions of parameters, (2) requires substantial GPU memory and time, (3) and is often unnecessary for relatively small classification tasks.\n",
        "\n",
        "Instead, we use **LoRA (Low-Rank Adaptation)**, a parameter-efficient fine-tuning method.\n",
        "\n",
        "LoRA inserts small, trainable low-rank matrices into the model’s attention layers, keeps the original pretrained weights frozen, and updates less than 1% of the total parameters.\n",
        "\n",
        "This keeps training fast and lightweight, while still allowing task-specific learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "grf05qgP2GMF",
      "metadata": {
        "id": "grf05qgP2GMF"
      },
      "source": [
        "## Environment Setup (LoRA fine-tuning)\n",
        "\n",
        "For fine-tuning, a GPU runtime is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7DSI22Tl2G5z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DSI22Tl2G5z",
        "outputId": "1f7de725-a3b2-4d02-96c0-0d5bff4f26ed"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip -q install transformers datasets evaluate accelerate peft\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3RnF8vm92Pk5",
      "metadata": {
        "id": "3RnF8vm92Pk5"
      },
      "source": [
        "## Load the same data and Prepare labels\n",
        "\n",
        "\n",
        "We reuse:\n",
        "- `media_bias_train.csv`\n",
        "- `media_bias_test.csv`\n",
        "\n",
        "Columns:\n",
        "- `text`: tweet text\n",
        "- `bias`: label\n",
        "\n",
        "\n",
        "Transformers expect integer labels: 0..K-1.\n",
        "We create a mapping from label name → id using the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7feGCJTD2A1B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "7feGCJTD2A1B",
        "outputId": "5959453a-d43f-4205-896c-e4614356fbd9"
      },
      "outputs": [],
      "source": [
        "# 1. Load Data\n",
        "train_df = pd.read_csv(\"media_bias_train.csv\")\n",
        "test_df  = pd.read_csv(\"media_bias_test.csv\")\n",
        "\n",
        "# 2. Prepare Label Mappings\n",
        "# We create a dictionary to map string labels to integers and vice versa.\n",
        "label_names = sorted(train_df[\"bias\"].unique())\n",
        "label2id = {name: i for i, name in enumerate(label_names)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "# 3. Apply mapping to DataFrames\n",
        "# We create a new column 'label' which the Trainer specifically looks for by default\n",
        "train_df[\"label\"] = train_df[\"bias\"].map(label2id)\n",
        "test_df[\"label\"]  = test_df[\"bias\"].map(label2id)\n",
        "\n",
        "# Quick check\n",
        "print(label2id)\n",
        "train_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "glYKy71h2U1-",
      "metadata": {
        "id": "glYKy71h2U1-"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Transformer models cannot read raw text directly.\n",
        "Instead, they rely on a built-in tokenizer that converts text into a numerical representation.\n",
        "\n",
        "Here, we simply:\n",
        "- pass each text through the tokenizer,\n",
        "- let it take care of formatting details internally,\n",
        "- and ensure texts are not too long for the model.\n",
        "\n",
        "No manual feature engineering is needed — the model handles this step for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qy92ZPFH2SpO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Qy92ZPFH2SpO",
        "outputId": "66d1a46d-eaa4-46a1-8a59-41be5258bebc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the base model - MiniLM (a small BERT-based model) to fit in memory/compute constraints\n",
        "base_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Tokenizes the text. Truncation=True ensures texts longer than max_length are cut off.\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=256, padding = \"max_length\")\n",
        "\n",
        "# Convert Pandas DataFrames to Hugging Face Datasets\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
        "\n",
        "# Apply tokenization\n",
        "# We remove the 'text' column because the model only needs the numerical 'input_ids'\n",
        "train_tok = train_ds.map(tokenize, batched=True).remove_columns([\"text\"])\n",
        "test_tok  = test_ds.map(tokenize, batched=True).remove_columns([\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AAZ2hI5Q24y1",
      "metadata": {
        "id": "AAZ2hI5Q24y1"
      },
      "source": [
        "## LoRA setup (PEFT)\n",
        "\n",
        "We load a MiniLM sequence classifier and apply LoRA adapters to attention projections.\n",
        "\n",
        "For MiniLM (which is BERT-based), common LoRA targets are `query` and `value`.\n",
        "This updates only a small number of parameters while leaving the base model frozen.\n",
        "\n",
        "\n",
        "\n",
        "**LoRA parameters**\n",
        "\n",
        "- `r`: the rank of the low-rank adapters.  \n",
        "  Higher values give the model more capacity to adapt, but add more trainable parameters.\n",
        "\n",
        "- `lora_alpha`: a scaling factor for the LoRA updates.  \n",
        "  It controls how strongly the adapters influence the original model weights.\n",
        "\n",
        "- `lora_dropout`: dropout applied inside the LoRA adapters during training.  \n",
        "  This helps regularization and can reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FXcHGYOR22LI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "551deb4b1fa74f29b06f84f2441d2126",
            "ea483aa7b94b447e90c1936bbb7d7f5d",
            "d96bc3c1db89434999dc0f9c14751491",
            "c6d30b4007a648799b8be0d380e2d95f",
            "4063fb300b934070a8fdd3c8fb2bb0bd",
            "1541b9f6f5794ad8b610c18bae964cb7",
            "33d68ac7da634dbeab0c4a5fc8a71e2f",
            "32e01617600b40e3a4aea2f39fb1669f",
            "cfc7437c44d644bf80181c1e438f5582",
            "99680e5b8c674ad7970b1407dfc23fcf",
            "b59efd1532ef42ffb263ae9b42eb7c70"
          ]
        },
        "id": "FXcHGYOR22LI",
        "outputId": "4b1c8f72-0e65-40da-e7a4-65ba18e9cfca"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=len(label_names),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# LoRA Configuration\n",
        "# LoRA works by adding pairs of rank-decomposition matrices to existing weights\n",
        "# and only training those newly added weights.\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # Sequence Classification\n",
        "    r=8,                        # Rank: The dimension of the low-rank matrices. Higher = more parameters.\n",
        "    lora_alpha=16,              # Alpha: Scaling factor. Usually set to 2x rank. Controls weight of adapter.\n",
        "    lora_dropout=0.05,          # Dropout probability for LoRA layers\n",
        "    target_modules=[\"query\", \"value\"], # Modules to apply LoRA to. For MiniLM/BERT, query/value is standard.\n",
        ")\n",
        "\n",
        "# Wrap the base model with the LoRA configuration\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Verify trainable parameters\n",
        "# You should see a very low percentage (usually <1%)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rv4Vv2gT3CbP",
      "metadata": {
        "id": "Rv4Vv2gT3CbP"
      },
      "source": [
        "## Training\n",
        "\n",
        "We train with HuggingFace `Trainer`.\n",
        "\n",
        "**Key training parameters**\n",
        "\n",
        "- `num_train_epochs`: how many times the model sees the full training dataset.  More epochs allow better learning but may lead to overfitting.\n",
        "\n",
        "- `learning_rate`: size of each update step during training.  Smaller values are more stable; larger values learn faster but can be unstable.\n",
        "\n",
        "- `logging_steps`: how often training progress is printed.\n",
        "\n",
        "- `save_strategy`: controls whether model checkpoints are saved during training.  Here we disable saving.\n",
        "\n",
        "- `report_to`: disables external logging tools (e.g., Weights & Biases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1EOe0vNN2_rr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300,
          "referenced_widgets": [
            "f5980f1f56984a389963764dbb78e2d4",
            "a43339acecf14cbf93132bdf4f95e73a",
            "230d35e3ccc24a4c9a19bda4b8a3937e",
            "f18ad0c14d1e44809ce6185850f65088",
            "af9d35bac52d414497e8291d61d96939",
            "93adff43ad0946718a668c764de4786c",
            "1b6d2c60e8ca46ae81ea9ca91e76af3f",
            "a1c9c01cf420427bac55d4d38aa5e057",
            "12beb16bab7f47c1866b9e58bc0238b4",
            "ff952cddba48436bb75d877aa308fe5a",
            "b99c9c9795ac416b8d5971662721d391"
          ]
        },
        "id": "1EOe0vNN2_rr",
        "outputId": "65eb2b94-0352-44be-8eb5-709ab769ee90"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_miniLM_b\",\n",
        "    per_device_train_batch_size=64,\n",
        "    logging_steps = 50,\n",
        "    num_train_epochs=15, # will take a bit of time\n",
        "    learning_rate=2e-4, # LoRA usually requires a higher LR\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # LoRA-augmented MiniLM model whose trainable parameters will be optimized\n",
        "    args=training_args,  # Training hyperparameters (learning rate, epochs, batch size, logging, etc.)\n",
        "    train_dataset=train_tok,  # Tokenized training data providing inputs and labels\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),  # Pads sequences dynamically per batch using the tokenizer\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tj3dVoh43ua0",
      "metadata": {
        "id": "Tj3dVoh43ua0"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "To fairly compare the base model and the LoRA-fine-tuned model, we must evaluate them using exactly the same procedure.\n",
        "\n",
        "Instead of writing our own evaluation loop, we use HuggingFace’s built-in **Trainer.evaluate()** method.\n",
        "\n",
        "The idea is simple:\n",
        "\n",
        "We keep the evaluation setup fixed and only change which model is being evaluated.\n",
        "\n",
        "To do this, we:\n",
        "\n",
        "\n",
        "*  Define how accuracy should be computed.\n",
        "\n",
        "*  Use a Trainer object to evaluate the base model.\n",
        "\n",
        "*  Use the same setup to evaluate the LoRA-fine-tuned model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AQ_E9JTetl5R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "33bdd59175bc4b059415a7b202ca180e",
            "753ce1b37e5742d59ab412aecdbd5827",
            "31cf5ff7dbaa4d00b0e9ab8da23ba2f4",
            "97d2d19e0cb1490da0d28dd98bf14e27",
            "c6d9fe7e55f8436693b141730f5eafb3",
            "faf58142b2af47dda81e0f30bd3d0383",
            "621ebadc955d4033baa26da1d96e58fc",
            "65bf6d6fdc7d430f84544ca6290cbf57",
            "49091d0ddf8647f1b89b9d0140533f55",
            "2a8aac1b9cf34b97962be075dcb43a5b",
            "50d2805e0ce24139a0b2deaf86b1f19c"
          ]
        },
        "id": "AQ_E9JTetl5R",
        "outputId": "7793bc18-57f2-430f-aafc-a08ad5b24e03"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "# Load a standard accuracy metric\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    This function tells the Trainer how to measure performance.\n",
        "\n",
        "    It receives:\n",
        "    - model outputs (logits)\n",
        "    - the correct labels\n",
        "\n",
        "    It returns:\n",
        "    - classification accuracy\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Convert model scores into predicted class labels\n",
        "    preds = logits.argmax(axis=-1)\n",
        "\n",
        "    # Compare predictions to true labels and compute accuracy\n",
        "    return accuracy_metric.compute(\n",
        "        predictions=preds,\n",
        "        references=labels\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDLJd21Lto_T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "TDLJd21Lto_T",
        "outputId": "832138ff-8970-49fb-8341-288fd905a5b8"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_name,\n",
        "    num_labels=len(label_names),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Create a Trainer for the base (not fine-tuned) model\n",
        "base_trainer = Trainer(\n",
        "    model=base_model,        # pretrained model with no task-specific adaptation\n",
        "    args=training_args,      # evaluation settings (batch size, device, etc.)\n",
        "    eval_dataset=test_tok,   # test data (never seen during training)\n",
        "    tokenizer=tokenizer,     # tokenizer used to prepare inputs\n",
        "    compute_metrics=compute_metrics,  # how to compute accuracy\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "base_metrics = base_trainer.evaluate()\n",
        "\n",
        "# Print baseline accuracy\n",
        "print(\"Accuracy before fine-tuning:\", base_metrics[\"eval_accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CbWg4JoIt1bn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "CbWg4JoIt1bn",
        "outputId": "4e1348d7-5bbe-4054-e423-ead2be66e677"
      },
      "outputs": [],
      "source": [
        "# Create a Trainer for the LoRA-fine-tuned model\n",
        "lora_trainer = Trainer(\n",
        "    model=trainer.model,     # model after LoRA fine-tuning\n",
        "    args=training_args,      # same evaluation settings\n",
        "    eval_dataset=test_tok,   # same test data\n",
        "    tokenizer=tokenizer,     # same tokenizer\n",
        "    compute_metrics=compute_metrics,  # same accuracy computation\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "lora_metrics = lora_trainer.evaluate()\n",
        "\n",
        "# Print accuracy after fine-tuning\n",
        "print(\"Accuracy after fine-tuning:\", lora_metrics[\"eval_accuracy\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-cee8mew38lj",
      "metadata": {
        "id": "-cee8mew38lj"
      },
      "source": [
        "**LoRA allows us to fine-tune a language model efficiently by training only a small number of additional parameters, while keeping the original model mostly fixed.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRfNESWs396f",
      "metadata": {
        "id": "qRfNESWs396f"
      },
      "source": [
        "**TASK:** LoRA sweep (capacity vs performance)\n",
        "Try:\n",
        "- `r ∈ {4, 8, 16}`\n",
        "- `lora_alpha ∈ {8, 16, 32}`\n",
        "\n",
        "How the performance changes? Why do you think it is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q1AIFOMaywHP",
      "metadata": {
        "id": "q1AIFOMaywHP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
