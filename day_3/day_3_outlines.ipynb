{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zLu4zgHZrA_"
      },
      "source": [
        "# Day 3\n",
        "Outlines example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYM6_Fx-Za0g",
        "outputId": "f64c1dfe-dc4c-44e5-d6c6-993ce3c56416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/LLM4BeSci_2025MetaRep/day_3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
        "    # Mount google drive to enable access to data files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Installing requisite packages\n",
        "    !pip install transformers accelerate outlines pydantic pymupdf &> /dev/null\n",
        "\n",
        "    # Change working directory to day_3\n",
        "    %cd /content/drive/MyDrive/LLM4BeSci_2025MetaRep/day_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dpPtqwSQZl0o"
      },
      "outputs": [],
      "source": [
        "import outlines\n",
        "from outlines.inputs import Chat\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdI8W5IgZqaj",
        "outputId": "c2813017-cd73-4524-d06a-557e706a87e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# huggingface token\n",
        "hf_token = \"YOUR_TOKEN\"\n",
        "\n",
        "# load text gen model and tokenizer\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\",\n",
        "                                                device_map=\"cuda\",\n",
        "                                                token = hf_token)\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\",\n",
        "                                             token = hf_token)\n",
        "\n",
        "# Create the Outlines model\n",
        "model = outlines.from_transformers(hf_model, hf_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwIBkFRCbwx0"
      },
      "source": [
        "## Creating a character\n",
        "Generic example using outlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6v415E_4Z-bw"
      },
      "outputs": [],
      "source": [
        "prompt = Chat([\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Create a character\"}\n",
        "    ])\n",
        "\n",
        "\n",
        "class Character(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    skills: List[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aogn8S-OaIWD",
        "outputId": "5b6fae5b-1fbc-4530-c289-59886a06fc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ \"name\": \"Elowen Blackwood\", \"age\":34, \"skills\": [\"Herbalism\", \"Tracking\", \"Woodcarving\", \"Basic First Aid\", \"Lockpicking\", \"Observational Skills\"] }\n"
          ]
        }
      ],
      "source": [
        "result = model(prompt, output_type=Character, max_new_tokens=200)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X5DaGjYb1aN"
      },
      "source": [
        "## Retrieving from PDF\n",
        "\n",
        "Applying to article PDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "6mDTG9GofGXF",
        "outputId": "154f98f4-a388-4222-9558-555819ac3a73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'123\\nrichly commented code is available in notebook format at\\ngithub.com/Zak-Hussain/LLM4BeSci.git, a GitHub reposi-\\ntory with instructions for running the code online in a Google\\nColab environment. The repository also provides a means of\\nkeepingthecodebaseforthistutorialuptodate.Keepinmind\\nthat the Hugging Face ecosystem is in active development,\\nmaking it likely that speciÔ¨Åc aspects of the code presented\\nin this paper will be deprecated by the time of reading. We\\nplan to regularly update the GitHub repository and respond to\\nupdate requests, which can be submitted as GitHub issues at\\ngithub.com/Zak-Hussain/LLM4BeSci/issues/new. For fur-\\nther information on Hugging Face, we suggest the Hugging\\nFace textbook by Tunstall et al. (2022).\\nFeature extraction\\nRelating personality measures\\nFeature extraction from LLMs is already being leveraged\\nin diverse ways to assist research in personality psychol-\\nogy (e.g., Abdurahman et al., 2023; Cutler & Condon, 2023;\\nWulff & Mata, 2023). In this examp'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract text from PDF\n",
        "pdf = fitz.open('llm_tutorial.pdf')\n",
        "tutorial = \"\"\n",
        "for page in pdf[8:11]:\n",
        "    tutorial += page.get_text()\n",
        "\n",
        "tutorial[:1000]  # Display the first 1000 characters to verify content extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2QOKfFNzgVWP"
      },
      "outputs": [],
      "source": [
        "prompt = Chat([\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"List the model names of the LLMs used in the exercises in the following article:\\n\\n\" + tutorial}\n",
        "    ])\n",
        "\n",
        "\n",
        "class LLMs(BaseModel):\n",
        "    model_names: List[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coZELZrajKx6",
        "outputId": "eca069e7-71d0-4570-9b49-dfb40d74287d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ \"model_names\": [\"distilbert\", \"BERT\"] }\n"
          ]
        }
      ],
      "source": [
        "result = model(prompt, output_type=LLMs, max_new_tokens=200)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
